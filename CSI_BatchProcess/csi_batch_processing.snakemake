#################################
#
# snakefile for converting CIDR Sequencing data deliveries to non-PII ready for GRIS upload
#
# Susan Huse, susan.huse@nih.gov
# Frederick National Lab
# April 10, 2019
#
#################################
## The following lines are to be included in the /bin/sh that calls snakemake with this file
## module load snakemake/5.3.0-Python-3.5.5
## module load Python/3.5.1-foss-2016a  # pandas doesn't work with the current version of python, need to load together
## module load pandas/0.18.0-foss-2016a-Python-3.5.1
#
## ln -s /hpcdata/dir/CIDR_DATA_RAW/BATCH${1}/Released_Data_Batch${1}/Holland_WES_Release_${1} rawdata
## csi_to_gris.py -b X -d "rawdata/Sample_Info" -need to set up the links to rawdata/Sample_Info/
## CLUSTER_OPTS="sbatch --cpus-per-task={cluster.threads} --mem={cluster.mem} --time={cluster.time} --partition={cluster.partition} --gres=lscratch:100"
## CLUSTER_OPTS="qsub -pe threaded {cluster.threads} -l h_vmem={cluster.mem} -l virtual_free={cluster.vmem} -wd $1/$2/$3"
## snakemake -j 71 --cluster-config cluster.json --cluster "$CLUSTER_OPTS" --keep-going > snakemake.output.txt 2>&1 &
## snakemake --rerun-incomplete --restart-times 10 -j ??? --keep-going --snakefile csi_batch_processing.snakemake > csi_batch_processing.log 2>&1

## 
## Load python modules
##
import os
from os.path import join
import pandas as pd
import re
import sys
from glob import glob
import datetime

##
## Set initial global variables
##
dir_renamed = os.getcwd()
dir_rawdata = join(dir_renamed, "rawdata")
dir_hla_master = "/sysapps/cluster/software/HLA-LA/1.0/HLA-LA-master"
batch_number = re.sub("^.*BATCH","",dir_renamed)
batch_name = "BATCH" + batch_number

dir_rawvcf = join(dir_rawdata, "MultiSampleVCF", "withGenotypeRefinement")
VCF = [f for f in os.listdir(dir_rawvcf) if re.match(r'.*.vcf.gz$', f)][0]
VCF = join(dir_rawvcf, VCF)
fnames = ["cumulative_coverage_counts", "cumulative_coverage_proportions", "gene_summary", "interval_statistics", "interval_summary", "statistics", "summary"]


## Set variables for rerunnin all of the old pedigrees
last_batch = str(int(batch_number) - 1)
dir_peds = "/hpcdata/dir/CIDR_DATA_RENAMED/pedigrees_previous"
todays_date = re.sub('-','',str(datetime.datetime.today()).split()[0])

##
## Read in the masterkey file 
##
df = pd.read_csv("masterkey_batch"+batch_number + ".txt", header=0, sep='\t')
df = df.loc[(df['Batch_Received'].isin([batch_name, ""])) | (df['Batch_Received'].isnull())]
dict_CIDR = dict(zip(df['Phenotips_ID'].tolist(), df['CIDR_Exome_ID'].tolist()))


##
## Set rule all
##
rule all:
    input: 
        #bam = expand(join(dir_renamed, "BAM", "{newID}.bam"), newID=list(dict_CIDR.keys())),
        #vcf = join(dir_renamed, "VCF", batch_name + ".vcf.gz"),
        #vci = join(dir_renamed, "VCF", batch_name + ".vcf.gz.tbi"),
        #target = expand(join(dir_renamed, "QC", "TARGET", "{newID}.TARGET.sample_{fname}.csv"), newID = list(dict_CIDR.keys()), fname=fnames),
        #ucsc = expand(join(dir_renamed, "QC", "UCSC", "{newID}.UCSC.sample_{fname}.csv"), newID = list(dict_CIDR.keys()), fname=fnames),
        #last_ped = join(dir_peds, todays_date, "seqr_ped_batch" + last_batch + ".txt"),
        hla = expand(join(dir_renamed, "HLA", "{newID}", "hla", "R1_bestguess_G.txt"), newID = list(dict_CIDR.keys())),
        admix_plot = join(dir_renamed, "BATCH_QC", "admixture", "admixture_mqc.png"),
        segs = expand(join(dir_renamed, "CNV", "gatk", "genotyped_segments_{newID}.vcf"), newID = list(dict_CIDR.keys())),
        intervals = expand(join(dir_renamed, "CNV", "gatk", "genotyped_intervals_{newID}.vcf"), newID = list(dict_CIDR.keys())),
        plota = join(dir_renamed, "inbreeding", "Heterozygous_to_Homozygous_Ratio_mqc.png"),
        plotb = join(dir_renamed, "inbreeding", "Mean_Homozygous_Tract_Length_mqc.png"),
        annot1 = expand(join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.intervals.annotations.tsv"), newID = list(dict_CIDR.keys())),
        annot2 = expand(join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.segments.annotations.tsv"), newID = list(dict_CIDR.keys())),
        
###
### Estimate ethnic admixture plot
###
rule admixture:
    input: 
        vcf = join(dir_renamed, "VCF", batch_name + ".vcf.gz"),
    output: 
#        admix_plot = join(dir_renamed, "BATCH_QC", "admixture", "admixture_mqc.png"),
        filtvcf = temp(join(dir_renamed, "BATCH_QC", "admixture", "filtered.recode.vcf")),
        mergedvcf = temp(join(dir_renamed, "BATCH_QC", "admixture", "merged.knowns.vcf")),
        admixtable = join(dir_renamed, "BATCH_QC", "admixture", "admixture_table.tsv"),
    params: 
        batch = batch_name, 
        rname = "admixture"
    shell: 
        """mkdir -p BATCH_QC/admixture
           module load vcftools/0.1.15-goolf-1.7.20-Perl-5.22.2
           vcftools --gzvcf {input.vcf} --remove-indels --max-missing 1 --recode --recode-INFO-all --out BATCH_QC/admixture/filtered
           module purge
           module load uge/8.5.5
           module load GATK/3.7-0-Java-1.8.0_92
           java -Xmx12g -jar $EBROOTGATK/GenomeAnalysisTK.jar -T CombineVariants -R /hpcdata/dir/CIDR_DATA_RENAMED/references/human_g1k_v37_decoy.fasta --genotypemergeoption UNSORTED -o {output.mergedvcf} --variant {output.filtvcf} --variant /hpcdata/dir/software/resources/1k_genomes_phase3_autosomes.vcf.gz --minimumN 2 -nt 4
           module purge
           module load uge/8.5.5
           module load plink/1.90-x86_64-beta
           plink --noweb --recode12 --snps-only --maf 0.05 --out BATCH_QC/admixture/merged.filtered.knowns --vcf {output.mergedvcf}
           perl /hpcdata/dir/software/admixture_prep.pl /hpcdata/dir/software/resources/1k_genomes_superpop_key.txt BATCH_QC/admixture/merged.filtered.knowns.pop BATCH_QC/admixture/merged.filtered.knowns.ped
           /hpcdata/dir/software/admixture_linux-1.3.0/admixture BATCH_QC/admixture/merged.filtered.knowns.ped 5 --supervised -j4
           mv merged.filtered.knowns.5.P BATCH_QC/admixture/merged.filtered.knowns.5.P
           mv merged.filtered.knowns.5.Q BATCH_QC/admixture/merged.filtered.knowns.5.Q
           perl /hpcdata/dir/software/admixture_post.pl /hpcdata/dir/software/resources/1k_genomes_superpop_key.txt {output.admixtable} BATCH_QC/admixture/merged.filtered.knowns.5.Q hg19 BATCH_QC/admixture/merged.filtered.knowns.ped"""

###
### Plot ethnic admixture
###
rule admixplot:
    input: admixtable = join(dir_renamed, "BATCH_QC", "admixture", "admixture_table.tsv"),
    output: admix_plot = join(dir_renamed, "BATCH_QC", "admixture", "admixture_mqc.png"),
    params: rname = "admixplot"
    shell: 
        """module load R; Rscript /hpcdata/dir/software/admixplot.R"""


###
### CNV Calling
###
rule cnv_wes:
    input: 
        bam = join(dir_renamed, "BAM", "{newID}.bam")
    output: 
        segs = join(dir_renamed, "CNV", "gatk", "genotyped_segments_{newID}.vcf"),
        intervals = join(dir_renamed, "CNV", "gatk", "genotyped_intervals_{newID}.vcf"),
    params: 
        batch = batch_name, 
        rname = "cnv"
    shell: 
        """/hpcdata/dir/software/gatk_cnvs_master.sh {wildcards.newID}"""

###
### CNV Post-processing
###
rule cnv_post:
    input: 
        vcf = join(dir_renamed, "VCF", batch_name + ".vcf.gz"),
        intervals = join(dir_renamed, "CNV", "gatk", "genotyped_intervals_{newID}.vcf"),
        segments = join(dir_renamed, "CNV", "gatk", "genotyped_segments_{newID}.vcf"),
    output: 
        reformat1 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "genotyped_intervals_{newID}.reformat.vcf")),
        reformat2 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "genotyped_segments_{newID}.reformat.vcf")),
        varintervals1 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "genotyped_intervals_{newID}.var.vcf")),
        varintervals2 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "genotyped_segments_{newID}.var.vcf")),
        cols1 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.intervals.columns")),
        cols2 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.segments.columns")),
        bed1 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.intervals.bed")),
        bed2 = temp(join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.segments.bed")),
        annot1 = join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.intervals.annotations.tsv"),
        annot2 = join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.segments.annotations.tsv"),
        candidates = join(dir_renamed, "CNV", "gatk", "{newID}", "{newID}.candidate.cnvs"),
    params: 
        batch = batch_name, 
        rname = "cnv_post",
        sample = "{newID}"
    shell: 
        """module load GATK/3.7-0-Java-1.8.0_92
           mkdir -p CNV/gatk/{params.sample}
           java -Xmx12g -jar $EBROOTGATK/GenomeAnalysisTK.jar -T SelectVariants -R /hpcdata/dir/CIDR_DATA_RENAMED/references/human_g1k_v37_decoy.fasta -V {input.intervals} -L /hpcdata/dir/software/resources/GRCh37.chroms.bed -o {output.reformat1} -nt 4
           java -Xmx12g -jar $EBROOTGATK/GenomeAnalysisTK.jar -T SelectVariants -R /hpcdata/dir/CIDR_DATA_RENAMED/references/human_g1k_v37_decoy.fasta -V {input.segments} -L /hpcdata/dir/software/resources/GRCh37.chroms.bed -o {output.reformat2} -nt 4
           module load bcftools
           bcftools view -q 1 {output.reformat1} > {output.varintervals1}
           bcftools view -q 1 {output.reformat2} > {output.varintervals2}
           bcftools query -f '%CHROM\t%POS\t%END[\t%CN]\n' {output.varintervals1} > {output.cols1}
           bcftools query -f '%CHROM\t%POS\t%END[\t%CN]\n' {output.varintervals2} > {output.cols2}
           sed -i '1 i\#Chrom\tStartPosition\tEndPosition\tCopyNumber' {output.cols1}
           sed -i '1 i\#Chrom\tStartPosition\tEndPosition\tCopyNumber' {output.cols2}
           perl /hpcdata/dir/software/make_CNV_bed.pl {output.cols1} {output.bed1}
           perl /hpcdata/dir/software/make_CNV_bed.pl {output.cols2} {output.bed2}
           export ANNOTSV=/hpcdata/dir/software/AnnotSV_2.1
           $ANNOTSV/bin/AnnotSV -genomeBuild GRCh37 -outputDir CNV/gatk/{params.sample} -outputFile CNV/gatk/{params.sample}/{params.sample}.intervals.annotations -SVinputFile {output.bed1} -svtBEDcol 5 -vcfFiles {input.vcf} -vcfSamples {params.sample} -bedtools /sysapps/cluster/software/BEDTools/2.27.1/bedtools
           $ANNOTSV/bin/AnnotSV -genomeBuild GRCh37 -outputDir CNV/gatk/{params.sample} -outputFile CNV/gatk/{params.sample}/{params.sample}.segments.annotations -SVinputFile {output.bed2} -svtBEDcol 5 -vcfFiles {input.vcf} -vcfSamples {params.sample} -bedtools /sysapps/cluster/software/BEDTools/2.27.1/bedtools
           perl /hpcdata/dir/software/filterCNVs.pl {output.annot1} {output.candidates}
        """

###
### Identify inbreeding outliers
###
rule inbreeding:
    input: 
        vcf = join(dir_renamed, "VCF", batch_name + ".vcf.gz"),
    output: 
        knownsvcf = temp(join(dir_renamed, "inbreeding", "known.sites.vcf")),
        filtvcf = temp(join(dir_renamed, "inbreeding", "filtered.known.sites.recode.vcf.gz")),
        plota = join(dir_renamed, "inbreeding", "Heterozygous_to_Homozygous_Ratio_mqc.png"),
        plotb = join(dir_renamed, "inbreeding", "Mean_Homozygous_Tract_Length_mqc.png"),
    params: 
        batch = batch_name, 
        rname = "inbreeding"
    shell: 
        """module load GATK/3.7-0-Java-1.8.0_92
           mkdir -p inbreeding
           java -Djava.io.tmpdir=inbreeding/tmp -jar $EBROOTGATK/GenomeAnalysisTK.jar -T SelectVariants -R /hpcdata/dir/CIDR_DATA_RENAMED/references/human_g1k_v37_decoy.fasta -L /hpcdata/dir/CIDR_DATA_RENAMED/references/exome_targets.bed -V {input.vcf} -o {output.knownsvcf} --concordance /hpcdata/dir/software/resources/1k_genomes_phase3_autosomes.vcf.gz -nt 4
           module purge
           module load uge/8.5.5
           module load vcftools/0.1.15-goolf-1.7.20-Perl-5.22.2
           vcftools --vcf {output.knownsvcf} --remove-indels --min-alleles 2 --max-alleles 2 --max-missing 1 --recode --recode-INFO-all --out inbreeding/filtered.known.sites
           module purge
           module load uge/8.5.5
           module load tabix
           bgzip inbreeding/filtered.known.sites.recode.vcf
           tabix -p vcf inbreeding/filtered.known.sites.recode.vcf.gz
           module purge
           module load uge/8.5.5
           module load picard/2.17.6
           java -jar /sysapps/cluster/software/picard/2.17.6/picard.jar CollectVariantCallingMetrics INPUT=inbreeding/filtered.known.sites.recode.vcf.gz OUTPUT=inbreeding/picardMetrics DBSNP=/hpcdata/dir/software/resources/00-All.vcf.gz THREAD_COUNT=8
           module purge
           module load uge/8.5.5
           module load plink/1.90-x86_64-beta
           plink --noweb --recode12 --snps-only --out inbreeding/filtered.known.sites --vcf inbreeding/filtered.known.sites.recode.vcf.gz
           module purge
           module load uge/8.5.5
           /hpcdata/dir/software/plink-1.07-x86_64/plink --file inbreeding/filtered.known.sites --noweb --homozyg --out inbreeding/ROH
           module purge
           module load uge/8.5.5
           module load R
           Rscript /hpcdata/dir/software/inbreedingPlot.R"""

##
## Rename the cram files using gatk and a sample mapping file and convert to bam
##
rule rename_bams:
    input: 
        cram = lambda w: expand(join(dir_rawdata, "CRAM", "{oldID}.cram"), oldID = dict_CIDR[w.newID])
    output:
        bam = join(dir_renamed, "BAM", "{newID}.bam") 
    params: 
        rname = "Rename_BAMS",
        oldsearch = lambda w: dict_CIDR[w.newID]
    shell:
        """module load samtools
        samtools view -H {input.cram} | sed -e 's/{params.oldsearch}/{wildcards.newID}/g' > {output.bam}.sam
        samtools view -b -o {output.bam}.tmp {input.cram}
        samtools reheader {output.bam}.sam {output.bam}.tmp  > {output.bam}
        rm {output.bam}.sam {output.bam}.tmp
        samtools index {output.bam}"""

###
### Rename the VCF file
###
### SUE: need to create one full vcf with tbi, and one batch specific vcf
rule rename_vcf:
    input: 
        vcf = VCF

    output:
        vcf = join(dir_renamed, "VCF", batch_name + ".vcf.gz"),
        vci = join(dir_renamed, "VCF", batch_name + ".vcf.gz.tbi"),
        mapa = temp(join(dir_renamed, "vcf_mapping_a.txt")),
        mapb = temp(join(dir_renamed, "vcf_mapping_b.txt")),
        first = temp(join(dir_renamed, "VCF", "first.vcf")),

    params:
        rname = "Rename_VCF",
        key = dir_renamed + "/masterkey_batch" + batch_number + ".txt",
        vcf_notgz = temp(join(dir_renamed, "VCF", batch_name + ".vcf")),

    shell:
        """module load bcftools/1.9-goolf-1.7.20
        tail -n +2 {params.key} | awk '{{print $3 "\\t" $2}}' > {output.mapa}
        cut -f 1 {output.mapa} > {output.mapb}
        bcftools view -S {output.mapb} --force-samples {input.vcf} > {output.first}
        bcftools reheader -s {output.mapa} -o {params.vcf_notgz} {output.first}
        bgzip {params.vcf_notgz}
        tabix -p vcf {output.vcf}"""
###
### Rename the QC files
### 
rule rename_qc:
    input:    
        target = lambda w: [join(dir_rawdata, "Gene_Exonic_Cvg_Reports", "TARGET",  dict_CIDR[w.newID] + ".TARGET_BED.sample_{fname}.csv")],
        ucsc = lambda w: [join(dir_rawdata, "Gene_Exonic_Cvg_Reports", "UCSC", dict_CIDR[w.newID] + ".UCSC_CODING.sample_{fname}.csv")]
    output: 
        target = join(dir_renamed, "QC", "TARGET", "{newID}.TARGET.sample_{fname}.csv"),
        ucsc = join(dir_renamed, "QC", "UCSC", "{newID}.UCSC.sample_{fname}.csv") 
    params:
        rname = "Rename_QC",
        oldsearch = lambda w: dict_CIDR[w.newID],
    shell: 
        """sed -e 's/{params.oldsearch}/{wildcards.newID}/g' {input.target} > {output.target}
        sed -e 's/{params.oldsearch}/{wildcards.newID}/g' {input.ucsc} > {output.ucsc}"""

###
### Create new pedigrees_previous directory and refresh all pedigree files
###
rule refresh_peds:
    output:
        # should create a list of ped files, but will die if any fail, so only need to test for last
        last_ped = join(dir_peds, todays_date, "seqr_ped_batch" + last_batch + ".txt")
    params:
        rname = "Refresh_Peds",
        dir_peds_today = join(dir_peds, todays_date),
        last_batch = last_batch
    shell:
        """module load python
        mkdir -p {params.dir_peds_today}
        cd {params.dir_peds_today}
        /hpcdata/dir/SCRIPTS/rerun_old_peds.sh {params.last_batch}"""

##
## Calculate HLA for all BAM files
##
rule hla:
    input: 
        bam = join(dir_renamed, "BAM", "{newID}.bam"),
    output: 
        hla = join(dir_renamed, "HLA", "{newID}", "hla", "R1_bestguess_G.txt"),
    params: 
        rname = "hla",
        hladir = join(dir_renamed, "HLA")
    shell: 
        """module load HLA-LA
        module load samtools
        mkdir -p HLA
        cd {dir_hla_master}
        ./HLA-LA.pl --BAM {input.bam} --graph ../../../../../../hpcdata/dir/CIDR_DATA_RENAMED/references/graphs/PRG_MHC_GRCh38_withIMGT  --sampleID {wildcards.newID} --maxThreads 4 --picard_sam2fastq_bin /sysapps/cluster/software/picardtools/1.119/SamToFastq.jar --workingDir {params.hladir}"""

